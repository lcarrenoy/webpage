[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Luis Carreño",
    "section": "",
    "text": "English  Português  Español\n\n\nAcademic Background: Technology professional with a degree in Industrial Engineering from UNI and specialization in Logistics from ESAN and TEC de Monterrey.\nExperience: Over 7 years solving complex problems through automation, data analysis, data engineering, and visualization.\nProgramming Languages: Python | Java | C++ | SQL | HTML and CSS.\nAutomation & Development: Power Automate | Make | N8N | Power Apps | Glide | Bubble | Flutter | Airtable\nSoftware & Tools: Power Query | R | Excel | BigQuery | PostgreSQL | Power BI | Tableau | Looker Studio | Google Cloud | OpenAI API | AI Google Studio | LangChain | AWS."
  },
  {
    "objectID": "Data_Engineer.html",
    "href": "Data_Engineer.html",
    "title": "Data Engineer",
    "section": "",
    "text": "This is an intensive 100-day program designed to train a Data Engineer from fundamentals to advanced projects. The plan includes key technologies, constant practice, and real-world projects."
  },
  {
    "objectID": "Data_Engineer.html#program-overview",
    "href": "Data_Engineer.html#program-overview",
    "title": "Data Engineer",
    "section": "",
    "text": "This is an intensive 100-day program designed to train a Data Engineer from fundamentals to advanced projects. The plan includes key technologies, constant practice, and real-world projects."
  },
  {
    "objectID": "Data_Engineer.html#program-structure",
    "href": "Data_Engineer.html#program-structure",
    "title": "Data Engineer",
    "section": "Program Structure",
    "text": "Program Structure\n\nPhase 1: Fundamentals (Days 1-13)\n\nSQL - Relational Databases (Days 1-4)\n\nDay 1: SQL introduction and basic commands (SELECT, FROM, WHERE)\nDay 2: Aggregation functions (SUM, AVG, COUNT), GROUP BY, HAVING, JOINs\nDay 3: Subqueries, CTEs, table creation, optimization\nDay 4: Practice: Database construction and complex queries\n\n\n\nPython for Data Engineering (Days 5-9)\n\nDay 5: Basic syntax, variables, control structures\nDay 6: Functions, modules, file manipulation (CSV, TXT)\nDay 7: Exception handling, libraries (pandas, numpy, json, requests)\nDay 8: Practice: Data cleaning and transformation with pandas\nDay 9: Project: API data extraction and CSV storage\n\n\n\nLinux and Bash (Days 10-13)\n\nDay 10: Linux basics, fundamental commands\nDay 11: Permissions, users, pipes, redirections, bash scripts, SSH\nDay 12: Practice: Cron jobs for periodic tasks\nDay 13: Practice: Bash script for file cleanup and organization\n\n#!/bin/bash\n# Example: Data processing bash script\n\nLOG_DIR=\"/var/log/data_pipeline\"\nDATA_DIR=\"/data/raw\"\nPROCESSED_DIR=\"/data/processed\"\n\n# Create log entry\nlog_message() {\n    echo \"$(date): $1\" &gt;&gt; \"$LOG_DIR/pipeline.log\"\n}\n\n# Process CSV files\nprocess_files() {\n    for file in \"$DATA_DIR\"/*.csv; do\n        if [ -f \"$file\" ]; then\n            filename=$(basename \"$file\")\n            log_message \"Processing $filename\"\n            \n            # Run Python processing script\n            python3 /scripts/process_data.py \"$file\" \"$PROCESSED_DIR/$filename\"\n            \n            if [ $? -eq 0 ]; then\n                log_message \"Successfully processed $filename\"\n                mv \"$file\" \"$DATA_DIR/archive/\"\n            else\n                log_message \"Error processing $filename\"\n            fi\n        fi\n    done\n}\n\nprocess_files\n\n\n\nPhase 2: Big Data and Advanced Tools (Days 14-23)\n\nBig Data and Hadoop (Days 14-16)\n\nDay 14: Big Data concepts, Hadoop, HDFS, MapReduce\nDay 15: Hadoop ecosystem (Pig, Hive, HBase), SQL queries in Big Data\nDay 16: Data modeling, Data Warehouse, normalization/denormalization\n\n\n\nData Pipelines with Apache Airflow (Days 17-20)\n\nDay 17: Pipeline introduction, Apache Airflow, DAGs\nDay 18: Connections, variables, operators, database integration\nDay 19: Practice: DAG for daily Python script execution\nDay 20: Project: Simple ETL flow with Airflow\n\n# Example: Airflow DAG for ETL pipeline\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndef extract_data(**context):\n    \"\"\"Extract data from source\"\"\"\n    # Implementation here\n    pass\n\ndef transform_data(**context):\n    \"\"\"Transform extracted data\"\"\"\n    # Implementation here\n    pass\n\ndef load_data(**context):\n    \"\"\"Load data to destination\"\"\"\n    # Implementation here\n    pass\n\ndag = DAG(\n    'daily_etl_pipeline',\n    default_args=default_args,\n    description='Daily ETL pipeline',\n    schedule_interval='@daily',\n    catchup=False\n)\n\nextract_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data,\n    dag=dag\n)\n\ntransform_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag\n)\n\nload_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag\n)\n\n# Set task dependencies\nextract_task &gt;&gt; transform_task &gt;&gt; load_task\n\n\nApache Spark (Days 21-23)\n\nDay 21: Spark architecture, installation, RDDs, DataFrames\nDay 22: DataFrame operations, Spark SQL\nDay 23: Practice: CSV file processing with Spark\n\n# Example: Spark DataFrame operations\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, avg, when, isnan, count\n\n# Initialize Spark Session\nspark = SparkSession.builder \\\n    .appName(\"DataProcessing\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Read data\ndf = spark.read.option(\"header\", \"true\").csv(\"path/to/data.csv\")\n\n# Data quality checks\ndef data_quality_report(df):\n    \"\"\"Generate data quality report\"\"\"\n    total_rows = df.count()\n    \n    quality_report = df.select([\n        count(when(isnan(c) | col(c).isNull(), c)).alias(c + \"_nulls\")\n        for c in df.columns\n    ]).collect()[0].asDict()\n    \n    return {\n        'total_rows': total_rows,\n        'null_counts': quality_report\n    }\n\n# Transformations\nprocessed_df = df \\\n    .filter(col(\"amount\") &gt; 0) \\\n    .withColumn(\"amount_category\", \n                when(col(\"amount\") &lt; 100, \"low\")\n                .when(col(\"amount\") &lt; 1000, \"medium\")\n                .otherwise(\"high\")) \\\n    .groupBy(\"category\", \"amount_category\") \\\n    .agg(\n        sum(\"amount\").alias(\"total_amount\"),\n        avg(\"amount\").alias(\"avg_amount\"),\n        count(\"*\").alias(\"transaction_count\")\n    )\n\n# Write results\nprocessed_df.write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"path/to/output\")\n\n\n\nPhase 3: Intermediate Projects (Days 24-32)\n\nSpecialized Practical Projects\nDay 24: Advanced SQL Project - Complex database design with multiple related tables - Implementation of complex queries with subqueries and CTEs - Query optimization with indexes and execution plan analysis - Creation of views, stored procedures, and triggers\nDay 25: Enterprise ETL Pipeline Objective: Create pipeline that extracts sales data from multiple sources - Sources: CSV files, relational databases, REST APIs - Transformations: Cleaning, standardization, data enrichment - Destination: Data Warehouse (Google BigQuery/Amazon Redshift) - Technologies: Python, pandas, SQL, cloud storage - Deliverables: Process documentation, versioned code, quality reports\nDay 26: Weather Data Project Use case: Real-time climate analysis system - Extraction: Weather API (OpenWeatherMap/Weather API) - Processing: Apache Spark for distributed transformations - Storage: SQL database optimized for time series - Features: Missing data handling, anomaly detection, temporal aggregations - Deliverables: Visualization dashboard, automatic alerts\nDay 27: Big Data Processing Challenge: Process CSV file &gt;1GB with Apache Spark - Dataset: Financial transaction data or server logs - Operations: Complex aggregations, multi-criteria filtering - Optimization: Partitioning, caching, Spark configuration - Storage: Data Warehouse with star schema - Metrics: Processing time, memory usage, throughput\nDay 28: Enterprise Report Automation Complete system: Automated daily sales reports - Source: SQL database with transactional data - Processing: Spark for aggregations and complex calculations - Orchestration: Apache Airflow for automatic scheduling - Outputs: PDF reports, web dashboards, email notifications - Features: Error handling, retries, detailed logs\nDays 29-32: Development and Refinement - Day 29: Integration of projects into personal portfolio - Day 30: Performance and scalability optimization - Day 31: Implementation of unit and integration tests - Day 32: Technical documentation and project presentations\n\n\n\nPhase 4: Advanced Technologies (Days 33-49)\n\nDistributed Systems and NoSQL (Days 33-36)\n\nDay 33: Distributed computing, distributed system architectures\nDay 34: NoSQL databases (MongoDB, Cassandra), Parquet/ORC formats\nDay 35: Practice: Migration of SQL project to NoSQL\nDay 36: Practice: Parquet format storage with Spark\n\n\n\nData Governance and Quality (Days 37-40)\n\nDay 37: Introduction to Data Governance, frameworks and responsibilities\nDay 38: Data Governance Framework\nDay 39: Data quality, Data Profiling, quality dimensions\nDay 40: Data Catalog, metadata management, taxonomies\n\n\n\nWeb Scraping (Days 41-49)\nFundamentals and Tools (Days 41-44) - Days 41-42: HTML/CSS, BeautifulSoup, Scrapy, form and session handling - Day 43: Data cleaning, cookie and session management, JavaScript handling - Day 44: Intelligent crawling, detection evasion, legal and ethical considerations\nPractical Web Scraping Projects (Days 45-49)\nDay 45: E-commerce Price Monitor Project: Product price monitoring system - Objective: Extract prices from multiple e-commerce sites - Technologies: Scrapy, requests, BeautifulSoup, pandas - Features: - Handle different HTML structures - User agent and proxy rotation - Price history storage - Change detection and alerts - Deliverables: Price database, trend dashboard\n# Example: Price monitoring scraper\nimport scrapy\nfrom scrapy.http import Request\nimport pandas as pd\nfrom datetime import datetime\n\nclass PriceSpider(scrapy.Spider):\n    name = 'price_monitor'\n    \n    def __init__(self, products_file=None):\n        self.products = pd.read_csv(products_file)\n        \n    def start_requests(self):\n        for _, product in self.products.iterrows():\n            yield Request(\n                url=product['url'],\n                callback=self.parse_price,\n                meta={'product_id': product['id'], 'name': product['name']}\n            )\n    \n    def parse_price(self, response):\n        # Extract price using CSS selectors\n        price_text = response.css('.price::text').get()\n        if price_text:\n            price = float(price_text.replace('$', '').replace(',', ''))\n            \n            yield {\n                'product_id': response.meta['product_id'],\n                'product_name': response.meta['name'],\n                'price': price,\n                'url': response.url,\n                'scraped_at': datetime.now(),\n                'availability': response.css('.availability::text').get()\n            }\nDay 46: News Aggregator System Project: Automatic news aggregator - Objective: Monitor multiple news sites and extract headlines - Functionalities: - Extract headlines, dates, categories - Automatic topic classification - Duplicate news detection - RSS feed integration - Technologies: Scrapy spiders, basic NLP, cron scheduling - Deliverables: News API, trend dashboard\nDays 47-49: Advanced Projects - Day 47: Social Media Analytics: Public social media data scraper - Day 48: Real Estate Monitor: Property monitoring system - Day 49: Job Market Analysis: Job posting extraction for market analysis\n\n\n\nPhase 5: Cloud Computing (Days 50-57)\n\nCloud Platforms (Days 50-57)\nServices covered: - AWS: S3, Lambda, Redshift, Kinesis, EMR, Glue, RDS - Google Cloud: BigQuery, Dataflow, Dataproc, Cloud Storage, Pub/Sub\n- Azure: Data Lake, SQL Database, Stream Analytics, HDInsight\nObjectives: - Design scalable infrastructures - Distributed storage - Real-time and batch processing\n# Example: AWS S3 data processing with Lambda\nimport boto3\nimport pandas as pd\nfrom io import StringIO\nimport json\n\ndef lambda_handler(event, context):\n    \"\"\"Process CSV files uploaded to S3\"\"\"\n    \n    s3_client = boto3.client('s3')\n    \n    # Get bucket and key from event\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n    \n    try:\n        # Read CSV from S3\n        response = s3_client.get_object(Bucket=bucket, Key=key)\n        csv_content = response['Body'].read().decode('utf-8')\n        \n        # Process with pandas\n        df = pd.read_csv(StringIO(csv_content))\n        \n        # Data transformations\n        processed_df = df.groupby('category').agg({\n            'amount': ['sum', 'mean', 'count'],\n            'date': ['min', 'max']\n        }).round(2)\n        \n        # Save processed data\n        output_key = f\"processed/{key.replace('.csv', '_processed.csv')}\"\n        s3_client.put_object(\n            Bucket=bucket,\n            Key=output_key,\n            Body=processed_df.to_csv(index=False)\n        )\n        \n        return {\n            'statusCode': 200,\n            'body': json.dumps(f'Successfully processed {key}')\n        }\n        \n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps(f'Error processing {key}: {str(e)}')\n        }\n\n\nData Streaming - Specialized Projects (Days 58-71)\nStreaming Fundamentals (Days 58-63) Tools: Apache Kafka, Apache Flink, Apache Storm, AWS Kinesis, Google Dataflow\nKey concepts: - Real-time vs batch data processing - Event-driven architectures and microservices - Windowing and temporal aggregations - Exactly-once processing and fault tolerance\nDays 64-67: Project 1 - Real-time Log Analysis System Use case: Enterprise web application monitoring\nDay 64: Architecture and Setup - Apache Kafka configuration as message broker - Apache Flink setup for stream processing - Elasticsearch configuration for storage - Pipeline design: Logs → Kafka → Flink → Elasticsearch\nDay 65: Ingestion and Processing - Implementation of producers for sending logs to Kafka - Development of Flink jobs for: - Log filtering by severity - Time window aggregations - Anomaly pattern detection - Data enrichment\n# Example: Kafka producer for log streaming\nfrom kafka import KafkaProducer\nimport json\nimport logging\nfrom datetime import datetime\n\nclass LogProducer:\n    def __init__(self, bootstrap_servers=['localhost:9092']):\n        self.producer = KafkaProducer(\n            bootstrap_servers=bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n            key_serializer=lambda k: k.encode('utf-8') if k else None\n        )\n    \n    def send_log(self, log_level, message, service_name, user_id=None):\n        \"\"\"Send log message to Kafka topic\"\"\"\n        log_entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': log_level,\n            'message': message,\n            'service': service_name,\n            'user_id': user_id\n        }\n        \n        topic = f\"logs-{log_level.lower()}\"\n        key = service_name\n        \n        try:\n            self.producer.send(topic, value=log_entry, key=key)\n            self.producer.flush()\n        except Exception as e:\n            logging.error(f\"Failed to send log: {e}\")\nDay 66: Alerts and Monitoring - Real-time alert system for critical errors - Real-time dashboard with Grafana - Performance metrics implementation - Automatic notification configuration\nDay 67: Optimization and Testing - Load testing with massive log generation - Throughput and latency optimization - Backpressure handling implementation - System documentation\nDays 68-71: Project 2 - Real-time Trading Platform Use case: Financial analysis system for algorithmic trading\nDay 68: Financial Data Ingestion - Integration with financial data APIs (Alpha Vantage, Yahoo Finance) - Kafka configuration for price streaming - Schema registry implementation for data evolution - Multiple topics setup per financial instrument\nDay 69: Trading Signal Processing - Real-time technical indicators implementation: - Moving averages (SMA, EMA) - RSI, MACD, Bollinger Bands - Trading pattern detection - Real-time risk calculation\nDay 70: Alert and Execution System - Buy/sell signal generation - Push alert system for opportunities - Order execution simulation - Portfolio performance tracking\nDay 71: Dashboard and Analysis - Interactive dashboard with real-time visualizations - Trading system performance metrics - Latency and throughput analysis - Simulated profitability reports\nTechnologies used in both projects: - Message Brokers: Apache Kafka, AWS Kinesis - Stream Processing: Apache Flink, Apache Storm - Storage: Elasticsearch, InfluxDB, AWS S3 - Visualization: Grafana, custom dashboards - Monitoring: Prometheus, custom metrics\n\n\n\nPhase 7: Final Project (Days 72-100)\n\nCapstone Project - Complete ETL Pipeline (Days 72-100)\nObjective: Develop an end-to-end data pipeline that solves a real business problem.\n\nStrategic Planning (Days 72-75)\nDay 72: Problem Identification and Objective Definition - Select a real use case (sales optimization, log analysis, real-time processing) - Define SMART project objectives - Identify success metrics and KPIs - Document the business problem to solve\nDay 73: Pipeline Architecture Design - Create high-level architecture diagram - Define data flow: sources → transformation → storage → consumption - Select technology stack (Python, Spark, Airflow, Cloud) - Design data schema and storage structure\nDay 74: Data Source Definition - Identify and catalog all data sources - Evaluate APIs, databases, CSV files, logs - Search for public datasets or generate simulated data if necessary - Document structure and format of each source\nDay 75: Infrastructure Configuration - Cloud: Configure AWS/GCP/Azure (S3/GCS, EMR/Dataproc, Redshift/BigQuery) - Local: Configure virtual environment with Python, Spark, Airflow - Establish connections between services - Configure security and permissions\n\n\nEnvironment Setup (Days 76-79)\nDay 76: Orchestration Configuration - Install and configure Apache Airflow - Create base DAG for task sequence - Configure connections and environment variables - Establish execution scheduling\nDay 77: Database Configuration - Configure SQL/NoSQL databases for storage - Create necessary schemas and tables - Configure indexes for optimization - Establish backup and recovery policies\nDay 78: Version Control and Documentation - Configure Git repository with proper structure - Create .gitignore and configuration files - Establish code and documentation conventions - Configure basic CI/CD\nDay 79: Monitoring Configuration - Implement detailed logging for each stage - Configure monitoring tools (Prometheus, Grafana) - Establish alerts and notifications - Create pipeline monitoring dashboards\n\n\nETL Development (Days 80-90)\nDays 80-82: Data Extraction (Extract) - Day 80: Python extraction script development - Day 81: Real-time API consumption implementation - Day 82: Initial cleaning and validation of extracted data\nDays 83-87: Data Transformation (Transform) - Day 83: Transformation rules definition (aggregations, filtering, normalization) - Day 84: Spark/pandas transformation development - Day 85: Data quality checks implementation - Day 86: Transformation testing and optimization - Day 87: Validation with large datasets\nDays 88-90: Data Loading (Load) - Day 88: Data Warehouse loading process configuration - Day 89: Complete ETL pipeline integration - Day 90: Airflow automation and execution scheduling\n\n\nOptimization and Finalization (Days 91-100)\nDays 91-93: Optimization and Security - Day 91: SQL query and Spark process optimization - Day 92: Security implementation (encryption, RBAC) - Day 93: Scalability and performance testing\nDays 94-96: Documentation and Visualization - Day 94: Complete pipeline and process documentation - Day 95: Dashboard creation with Tableau/Power BI - Day 96: Final results report preparation\nDays 97-100: Review and Presentation - Day 97: Complete review and error correction - Day 98: End-to-end testing - Day 99: Final presentation preparation - Day 100: Project presentation and final delivery"
  },
  {
    "objectID": "Data_Engineer.html#technologies-and-tools-covered",
    "href": "Data_Engineer.html#technologies-and-tools-covered",
    "title": "Data Engineer",
    "section": "Technologies and Tools Covered",
    "text": "Technologies and Tools Covered\n\nProgramming Languages\n\nSQL - Queries, optimization, database management\nPython - Data manipulation, automation, APIs\nBash - Automation, system administration\n\n\n\nBig Data Tools\n\nApache Hadoop - HDFS, MapReduce, complete ecosystem\nApache Spark - Distributed processing, DataFrames, SQL\nApache Airflow - Pipeline orchestration, DAGs\n\n\n\nDatabases\n\nRelational - MySQL, PostgreSQL, optimization\nNoSQL - MongoDB, Cassandra\nData Warehouses - BigQuery, Redshift\n\n\n\nCloud Platforms\n\nAWS - S3, EMR, Redshift, Kinesis, Lambda, Glue\nGoogle Cloud - BigQuery, Dataflow, Dataproc\nMicrosoft Azure - Data Lake, Stream Analytics\n\n\n\nStreaming and Real-time\n\nApache Kafka - Message streaming\nApache Flink - Stream processing\nAWS Kinesis - Real-time data streaming"
  },
  {
    "objectID": "Data_Engineer.html#learning-methodology",
    "href": "Data_Engineer.html#learning-methodology",
    "title": "Data Engineer",
    "section": "Learning Methodology",
    "text": "Learning Methodology\n\nDaily Structure\n\nTheory - Fundamental concepts\nGuided Practice - Structured exercises\n\nProjects - Practical application\nReview - Knowledge consolidation\n\n\n\nProgressive Approach\n\nWeeks 1-2: Solid fundamentals\nWeeks 3-7: Intermediate tools\nWeeks 8-11: Advanced technologies\nWeeks 12-14: Independent project\n\n\n\nKey Components\n\n40% Theory - Concepts and best practices\n35% Practice - Exercises and labs\n25% Projects - Real application"
  },
  {
    "objectID": "Data_Engineer.html#expected-results",
    "href": "Data_Engineer.html#expected-results",
    "title": "Data Engineer",
    "section": "Expected Results",
    "text": "Expected Results\nUpon completing the 100 days, you will have:\n\nTechnical Skills\n\nSQL mastery for complex data analysis\nPython competency for data engineering\nPractical experience with Big Data tools\nKnowledge of scalable cloud architectures\nAbility to design robust ETL pipelines\n\n\n\nPortfolio Projects\nUpon completing the program, you will have a robust portfolio with diverse projects:\n\nFundamental Projects\n\nEnterprise ETL Pipeline: Complete system for extracting, transforming, and loading sales data\nAdvanced SQL Project: Optimized database with complex queries and stored procedures\nClimate Monitoring System: API integration with distributed Spark processing\n\n\n\nBig Data Projects\n\nMassive File Processor: Handling datasets &gt;1GB with Spark optimizations\nAutomated Reporting System: Airflow orchestration for enterprise reports\nSQL to NoSQL Migration: Performance comparison between relational and NoSQL databases\n\n\n\nWeb Scraping Projects\n\nE-commerce Price Monitor: Price monitoring system with automatic alerts\nNews Aggregator: News aggregation platform with automatic classification\nReal Estate Analytics: Real estate market analysis with extracted data\n\n\n\nCloud Projects\n\nMulti-Cloud Architecture: Implementation on AWS, GCP, and Azure with service comparison\nData Lake Implementation: Distributed storage with serverless processing\nStreaming Analytics: Real-time processing with cloud-native services\n\n\n\nStreaming Projects\n\nLog Analysis System: Real-time monitoring of enterprise applications\nTrading Platform: Financial analysis with real-time technical indicators\nIoT Data Pipeline: Sensor data processing with Apache Kafka and Flink\n\n\n\nCapstone Project\n\nEnd-to-End ETL Pipeline: Complete solution integrating all learned technologies\nProfessional Documentation: Architecture, code, tests, and performance metrics\nExecutive Presentation: Demonstration of business value and project ROI\n\n\n\n\nProfessional Preparation\n\nKnowledge of industry best practices\nExperience with standard enterprise tools\n\nAbility to solve real data problems\nSolid foundation for Data Engineer roles"
  },
  {
    "objectID": "Data_Engineer.html#success-recommendations",
    "href": "Data_Engineer.html#success-recommendations",
    "title": "Data Engineer",
    "section": "Success Recommendations",
    "text": "Success Recommendations\n\nConsistency\n\nDedicate daily time to the program\nMaintain a sustainable pace\nDon’t skip practice sessions\n\n\n\nActive Practice\n\nImplement all exercises\nExperiment beyond requirements\nDocument your progress\n\n\n\nCommunity\n\nShare your projects\nSeek feedback from other professionals\nParticipate in Data Engineering communities\n\n\n\nAdaptation\n\nAdjust pace according to your availability\nDeepen areas of greater interest\nMaintain focus on practical objectives\n\n\nTotal Duration: 100 days\nLevel: Beginner to Intermediate-Advanced\nMode: Self-study with practical projects\nResult: Complete preparation for Data Engineer roles"
  },
  {
    "objectID": "about-pt.html",
    "href": "about-pt.html",
    "title": "Luis Carreño",
    "section": "",
    "text": "English  Português  Español\n\n\nFormação Acadêmica: Profissional em tecnologia com formação em Engenharia Industrial pela UNI e especialização em Logística pela ESAN e TEC de Monterrey.\nExperiência: Mais de 7 anos resolvendo problemas complexos através de automação, análise de dados, engenharia de dados e visualização.\nLinguagens de Programação: Python | Java | C++ | SQL | HTML e CSS.\nAutomação e Desenvolvimento: Power Automate | Make | N8N | Power Apps | Glide | Bubble | Flutter | Airtable\nSoftware e Ferramentas: Power Query | R | Excel | BigQuery | PostgreSQL | Power BI | Tableau | Looker Studio | Google Cloud | OpenAI API | AI Google Studio | LangChain | AWS."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luis Carreño",
    "section": "",
    "text": "English  Português  Español\n\n\nAcademic Background: Technology professional with a degree in Industrial Engineering from UNI and specialization in Logistics from ESAN and TEC de Monterrey.\nExperience: Over 7 years solving complex problems through automation, data analysis, data engineering, and visualization.\nProgramming Languages: Python | Java | C++ | SQL | HTML and CSS.\nAutomation & Development: Power Apps | Power Automate | Make | Glide | Airtable | N8N | Bubble | Flutter\nSoftware & Tools: Power Query | R | Excel | BigQuery | PostgreSQL | Power BI | Tableau | Looker Studio | Google Cloud | OpenAI API | AI Google Studio | LangChain | AWS."
  },
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Blog",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 30, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 27, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog 2\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 27, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Science.html",
    "href": "Data_Science.html",
    "title": "Data Science",
    "section": "",
    "text": "This document outlines essential tools and structured learning paths for Data Scientists, focusing on statistical analysis, machine learning, and big data processing.\n\n\n\n\n\nFunction: Statistical programming language designed for data analysis, statistical modeling, and data visualization with an extensive package ecosystem.\nWebsite: R Project\nCost Model: Open Source\nBest For: Statistical analysis, academic research, data visualization\n\n\n\n\n\n\n\nWhy R?\n\n\n\nR es especialmente potente para análisis estadístico y visualización de datos, con una comunidad académica muy activa.\n\n\n\n\n\n\nFunction: General-purpose programming language with powerful data science libraries (scikit-learn, pandas, numpy) for machine learning and data analysis.\nWebsite: Python.org\nCost Model: Open Source\nBest For: Machine learning, automation, production deployment\n\n\n\n\n\n\n\nWhy Python?\n\n\n\nPython ofrece versatilidad y facilidad de integración en sistemas de producción, siendo ideal para machine learning y automatización.\n\n\n\n\n\n\n\n\n\nFunction: Standard language for managing and querying relational databases, essential for data extraction and manipulation in data science workflows.\nWebsite: W3Schools SQL\nCost Model: Free (varies by database system)\nBest For: Data extraction, database management, data preprocessing\n\n-- Ejemplo básico de consulta SQL\nSELECT column1, column2, COUNT(*) as count\nFROM table_name\nWHERE condition = 'value'\nGROUP BY column1, column2\nORDER BY count DESC;\n\n\n\n\n\n\n\nFunction: Distributed version control system for tracking changes in code, enabling collaboration and reproducible data science projects.\nWebsite: Git\nCost Model: Open Source\nBest For: Code versioning, collaboration, project management\n\n# Comandos básicos de Git\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit push origin main\n\n\n\n\n\n\n\nFunction: Unified analytics engine for large-scale data processing with MLlib for machine learning at scale.\nWebsite: Apache Spark\nCost Model: Open Source\nBest For: Large-scale data processing, distributed machine learning\n\n# Ejemplo básico con PySpark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"DataScienceExample\") \\\n    .getOrCreate()\n\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\ndf.show()\n\n\n\n\n\n\n\nCore FoundationDevelopment SkillsBusiness Applications\n\n\n\nIntermediate Importing Data in R\nIntroduction to SQL\nIntermediate SQL\nJoining Data in SQL\n\n\n\n\nDeveloping R Packages\nIntroduction to Git\nIntermediate Git\n\n\n\n\nMachine Learning for Business\nFeature Engineering in R\n\n\n\n\n\n\n\n\nCore FoundationDevelopment SkillsBusiness Applications\n\n\n\nIntermediate Importing Data in Python\nIntroduction to SQL\nIntermediate SQL\nJoining Data in SQL\n\n\n\n\nDeveloping Python Packages\nIntroduction to Git\nIntermediate Git\n\n\n\n\nPreprocessing for Machine Learning in Python\nMachine Learning for Business\n\n\n\n\n\n\n\n\n\n\n\nSupervised LearningUnsupervised LearningAdvanced TechniquesBayesian MethodsBig Data\n\n\n\nSupervised Learning in R: Classification\nSupervised Learning in R: Regression\nIntermediate Regression in R\nMachine Learning with caret in R\nModeling with tidymodels in R\n\n\n\n\nUnsupervised Learning in R\nCluster Analysis in R\nDimensionality Reduction in R\n\n\n\n\nMachine Learning in the Tidyverse\nMachine Learning with Tree-Based Models in R\nSupport Vector Machines in R\nHyperparameter Tuning in R\n\n\n\n\nFundamentals of Bayesian Data Analysis in R\nBayesian Regression Modeling with rstanarm\n\n\n\n\nIntroduction to Spark with sparklyr in R\n\n\n\n\n\n\n\n\nCore Machine LearningAdvanced Algorithms\n\n\n\nSupervised Learning with scikit-learn\nUnsupervised Learning in Python\nLinear Classifiers in Python\n\n\n\n\nMachine Learning with Tree-Based Models in Python\nExtreme Gradient Boosting with XGBoost\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nTools\n\n\n\n\nCore\nR, RStudio, tidyverse\n\n\nVisualization\nggplot2, plotly, shiny\n\n\nModeling\ncaret, tidymodels, randomForest\n\n\nBayesian\nrstanarm, brms, MCMCpack\n\n\n\n# Ejemplo de flujo típico en R\nlibrary(tidyverse)\nlibrary(caret)\n\n# Cargar y explorar datos\ndata &lt;- read_csv(\"dataset.csv\")\ndata %&gt;% \n  glimpse() %&gt;%\n  summary()\n\n# Modelado\nmodel &lt;- train(target ~ ., \n               data = data,\n               method = \"rf\",\n               trControl = trainControl(method = \"cv\"))\n\n\n\n\n\n\nCategory\nTools\n\n\n\n\nCore\nPython, Jupyter, pandas, numpy\n\n\nVisualization\nmatplotlib, seaborn, plotly\n\n\nModeling\nscikit-learn, XGBoost, TensorFlow\n\n\nDeep Learning\nPyTorch, Keras, TensorFlow\n\n\n\n# Ejemplo de flujo típico en Python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Cargar y explorar datos\ndf = pd.read_csv('dataset.csv')\nprint(df.info())\n\n# Preparar datos\nX = df.drop('target', axis=1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Modelado\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\n\n\n\n\n\nCategory\nTools\n\n\n\n\nDatabases\nPostgreSQL, MySQL, MongoDB\n\n\nBig Data\nSpark, Hadoop, Kafka\n\n\nCloud\nAWS, Azure, Google Cloud\n\n\nContainers\nDocker, Kubernetes\n\n\n\n\n\n\n\n\n\nIntegración de Herramientas\n\n\n\nLa elección de herramientas debe considerar el ecosistema completo: desde la ingesta de datos hasta el despliegue en producción.\n\n\n\n\n\n\n\n\n\nEmpezar con fundamentos: SQL y estadística básica\nElegir un lenguaje principal: R para análisis estadístico, Python para ML\nPracticar con proyectos reales: Kaggle, proyectos personales\nAprender control de versiones: Git desde el inicio\n\n\n\n\n\nEspecializarse: Machine Learning o Análisis Estadístico\nAprender herramientas de producción: Docker, Cloud platforms\nContribuir a proyectos open source\nNetworking: Comunidades de Data Science\n\n\n\n\n\nDeep Learning y AI: TensorFlow, PyTorch\nBig Data: Spark, Hadoop ecosystems\nMLOps: Despliegue y monitoreo de modelos\nLiderazgo técnico: Arquitectura de datos, gestión de equipos"
  },
  {
    "objectID": "Data_Science.html#core-programming-languages",
    "href": "Data_Science.html#core-programming-languages",
    "title": "Data Science",
    "section": "",
    "text": "Function: Statistical programming language designed for data analysis, statistical modeling, and data visualization with an extensive package ecosystem.\nWebsite: R Project\nCost Model: Open Source\nBest For: Statistical analysis, academic research, data visualization\n\n\n\n\n\n\n\nWhy R?\n\n\n\nR es especialmente potente para análisis estadístico y visualización de datos, con una comunidad académica muy activa.\n\n\n\n\n\n\nFunction: General-purpose programming language with powerful data science libraries (scikit-learn, pandas, numpy) for machine learning and data analysis.\nWebsite: Python.org\nCost Model: Open Source\nBest For: Machine learning, automation, production deployment\n\n\n\n\n\n\n\nWhy Python?\n\n\n\nPython ofrece versatilidad y facilidad de integración en sistemas de producción, siendo ideal para machine learning y automatización."
  },
  {
    "objectID": "Data_Science.html#database-query-tools",
    "href": "Data_Science.html#database-query-tools",
    "title": "Data Science",
    "section": "",
    "text": "Function: Standard language for managing and querying relational databases, essential for data extraction and manipulation in data science workflows.\nWebsite: W3Schools SQL\nCost Model: Free (varies by database system)\nBest For: Data extraction, database management, data preprocessing\n\n-- Ejemplo básico de consulta SQL\nSELECT column1, column2, COUNT(*) as count\nFROM table_name\nWHERE condition = 'value'\nGROUP BY column1, column2\nORDER BY count DESC;"
  },
  {
    "objectID": "Data_Science.html#version-control",
    "href": "Data_Science.html#version-control",
    "title": "Data Science",
    "section": "",
    "text": "Function: Distributed version control system for tracking changes in code, enabling collaboration and reproducible data science projects.\nWebsite: Git\nCost Model: Open Source\nBest For: Code versioning, collaboration, project management\n\n# Comandos básicos de Git\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit push origin main"
  },
  {
    "objectID": "Data_Science.html#big-data-processing",
    "href": "Data_Science.html#big-data-processing",
    "title": "Data Science",
    "section": "",
    "text": "Function: Unified analytics engine for large-scale data processing with MLlib for machine learning at scale.\nWebsite: Apache Spark\nCost Model: Open Source\nBest For: Large-scale data processing, distributed machine learning\n\n# Ejemplo básico con PySpark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"DataScienceExample\") \\\n    .getOrCreate()\n\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\ndf.show()"
  },
  {
    "objectID": "Data_Science.html#data-scientist-learning-paths",
    "href": "Data_Science.html#data-scientist-learning-paths",
    "title": "Data Science",
    "section": "",
    "text": "Core FoundationDevelopment SkillsBusiness Applications\n\n\n\nIntermediate Importing Data in R\nIntroduction to SQL\nIntermediate SQL\nJoining Data in SQL\n\n\n\n\nDeveloping R Packages\nIntroduction to Git\nIntermediate Git\n\n\n\n\nMachine Learning for Business\nFeature Engineering in R\n\n\n\n\n\n\n\n\nCore FoundationDevelopment SkillsBusiness Applications\n\n\n\nIntermediate Importing Data in Python\nIntroduction to SQL\nIntermediate SQL\nJoining Data in SQL\n\n\n\n\nDeveloping Python Packages\nIntroduction to Git\nIntermediate Git\n\n\n\n\nPreprocessing for Machine Learning in Python\nMachine Learning for Business"
  },
  {
    "objectID": "Data_Science.html#machine-learning-specialist-paths",
    "href": "Data_Science.html#machine-learning-specialist-paths",
    "title": "Data Science",
    "section": "",
    "text": "Supervised LearningUnsupervised LearningAdvanced TechniquesBayesian MethodsBig Data\n\n\n\nSupervised Learning in R: Classification\nSupervised Learning in R: Regression\nIntermediate Regression in R\nMachine Learning with caret in R\nModeling with tidymodels in R\n\n\n\n\nUnsupervised Learning in R\nCluster Analysis in R\nDimensionality Reduction in R\n\n\n\n\nMachine Learning in the Tidyverse\nMachine Learning with Tree-Based Models in R\nSupport Vector Machines in R\nHyperparameter Tuning in R\n\n\n\n\nFundamentals of Bayesian Data Analysis in R\nBayesian Regression Modeling with rstanarm\n\n\n\n\nIntroduction to Spark with sparklyr in R\n\n\n\n\n\n\n\n\nCore Machine LearningAdvanced Algorithms\n\n\n\nSupervised Learning with scikit-learn\nUnsupervised Learning in Python\nLinear Classifiers in Python\n\n\n\n\nMachine Learning with Tree-Based Models in Python\nExtreme Gradient Boosting with XGBoost"
  },
  {
    "objectID": "Data_Science.html#tool-ecosystem-by-specialization",
    "href": "Data_Science.html#tool-ecosystem-by-specialization",
    "title": "Data Science",
    "section": "",
    "text": "Category\nTools\n\n\n\n\nCore\nR, RStudio, tidyverse\n\n\nVisualization\nggplot2, plotly, shiny\n\n\nModeling\ncaret, tidymodels, randomForest\n\n\nBayesian\nrstanarm, brms, MCMCpack\n\n\n\n# Ejemplo de flujo típico en R\nlibrary(tidyverse)\nlibrary(caret)\n\n# Cargar y explorar datos\ndata &lt;- read_csv(\"dataset.csv\")\ndata %&gt;% \n  glimpse() %&gt;%\n  summary()\n\n# Modelado\nmodel &lt;- train(target ~ ., \n               data = data,\n               method = \"rf\",\n               trControl = trainControl(method = \"cv\"))\n\n\n\n\n\n\nCategory\nTools\n\n\n\n\nCore\nPython, Jupyter, pandas, numpy\n\n\nVisualization\nmatplotlib, seaborn, plotly\n\n\nModeling\nscikit-learn, XGBoost, TensorFlow\n\n\nDeep Learning\nPyTorch, Keras, TensorFlow\n\n\n\n# Ejemplo de flujo típico en Python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Cargar y explorar datos\ndf = pd.read_csv('dataset.csv')\nprint(df.info())\n\n# Preparar datos\nX = df.drop('target', axis=1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Modelado\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\n\n\n\n\n\nCategory\nTools\n\n\n\n\nDatabases\nPostgreSQL, MySQL, MongoDB\n\n\nBig Data\nSpark, Hadoop, Kafka\n\n\nCloud\nAWS, Azure, Google Cloud\n\n\nContainers\nDocker, Kubernetes\n\n\n\n\n\n\n\n\n\nIntegración de Herramientas\n\n\n\nLa elección de herramientas debe considerar el ecosistema completo: desde la ingesta de datos hasta el despliegue en producción."
  },
  {
    "objectID": "Data_Science.html#recomendaciones-de-aprendizaje",
    "href": "Data_Science.html#recomendaciones-de-aprendizaje",
    "title": "Data Science",
    "section": "",
    "text": "Empezar con fundamentos: SQL y estadística básica\nElegir un lenguaje principal: R para análisis estadístico, Python para ML\nPracticar con proyectos reales: Kaggle, proyectos personales\nAprender control de versiones: Git desde el inicio\n\n\n\n\n\nEspecializarse: Machine Learning o Análisis Estadístico\nAprender herramientas de producción: Docker, Cloud platforms\nContribuir a proyectos open source\nNetworking: Comunidades de Data Science\n\n\n\n\n\nDeep Learning y AI: TensorFlow, PyTorch\nBig Data: Spark, Hadoop ecosystems\nMLOps: Despliegue y monitoreo de modelos\nLiderazgo técnico: Arquitectura de datos, gestión de equipos"
  },
  {
    "objectID": "Statistics.html",
    "href": "Statistics.html",
    "title": "Stastistics",
    "section": "",
    "text": "Statistician\n\nFunction: Professional role combining statistical theory, R programming, and data analysis expertise for research and business insights\nWebsite: https://www.amstat.org\nCost Model: Career Path\n\nIntroduction to Statistics in R\n\nFunction: Comprehensive introduction to statistical concepts and R programming fundamentals for data analysis and visualization\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFoundations of Probability in R\n\nFunction: Core probability theory, distributions, and random variables using R for statistical modeling and inference\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFoundations of Inference in R\n\nFunction: Statistical inference principles including confidence intervals, hypothesis testing, and p-values using R programming\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\n\n\n\nIntroduction to Regression in R\n\nFunction: Linear regression fundamentals, model building, diagnostics, and interpretation using R statistical packages\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nIntermediate Regression in R\n\nFunction: Advanced regression techniques including multiple regression, polynomial regression, and model selection strategies\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nHierarchical and Mixed Effects Models in R\n\nFunction: Multi-level modeling, random effects, and nested data analysis using lme4 and similar R packages\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\n\n\n\nSampling in R\n\nFunction: Sampling theory, survey design, and sampling distributions with practical R implementations for data collection\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nHypothesis Testing in R\n\nFunction: Statistical hypothesis testing, t-tests, ANOVA, chi-square tests, and non-parametric methods using R\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nExperimental Design in R\n\nFunction: Design of experiments, randomization, blocking, factorial designs, and analysis using R statistical tools\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nAnalyzing Survey Data in R\n\nFunction: Survey data analysis techniques, weighting, complex sampling designs, and survey package utilization in R\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\n\n\n\nSurvival Analysis in R\n\nFunction: Time-to-event analysis, Kaplan-Meier curves, Cox regression, and survival modeling using R survival packages\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFundamentals of Bayesian Data Analysis in R\n\nFunction: Bayesian inference, MCMC methods, prior distributions, and posterior analysis using R and Stan/JAGS\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFactor Analysis in R\n\nFunction: Exploratory and confirmatory factor analysis, dimensionality reduction, and latent variable modeling in R\nWebsite: https://www.datacamp.com\nCost Model: Subscription"
  },
  {
    "objectID": "Statistics.html#foundational-statistics",
    "href": "Statistics.html#foundational-statistics",
    "title": "Stastistics",
    "section": "",
    "text": "Statistician\n\nFunction: Professional role combining statistical theory, R programming, and data analysis expertise for research and business insights\nWebsite: https://www.amstat.org\nCost Model: Career Path\n\nIntroduction to Statistics in R\n\nFunction: Comprehensive introduction to statistical concepts and R programming fundamentals for data analysis and visualization\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFoundations of Probability in R\n\nFunction: Core probability theory, distributions, and random variables using R for statistical modeling and inference\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFoundations of Inference in R\n\nFunction: Statistical inference principles including confidence intervals, hypothesis testing, and p-values using R programming\nWebsite: https://www.datacamp.com\nCost Model: Subscription"
  },
  {
    "objectID": "Statistics.html#regression-modeling",
    "href": "Statistics.html#regression-modeling",
    "title": "Stastistics",
    "section": "",
    "text": "Introduction to Regression in R\n\nFunction: Linear regression fundamentals, model building, diagnostics, and interpretation using R statistical packages\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nIntermediate Regression in R\n\nFunction: Advanced regression techniques including multiple regression, polynomial regression, and model selection strategies\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nHierarchical and Mixed Effects Models in R\n\nFunction: Multi-level modeling, random effects, and nested data analysis using lme4 and similar R packages\nWebsite: https://www.datacamp.com\nCost Model: Subscription"
  },
  {
    "objectID": "Statistics.html#specialized-analysis-methods",
    "href": "Statistics.html#specialized-analysis-methods",
    "title": "Stastistics",
    "section": "",
    "text": "Sampling in R\n\nFunction: Sampling theory, survey design, and sampling distributions with practical R implementations for data collection\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nHypothesis Testing in R\n\nFunction: Statistical hypothesis testing, t-tests, ANOVA, chi-square tests, and non-parametric methods using R\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nExperimental Design in R\n\nFunction: Design of experiments, randomization, blocking, factorial designs, and analysis using R statistical tools\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nAnalyzing Survey Data in R\n\nFunction: Survey data analysis techniques, weighting, complex sampling designs, and survey package utilization in R\nWebsite: https://www.datacamp.com\nCost Model: Subscription"
  },
  {
    "objectID": "Statistics.html#advanced-statistical-methods",
    "href": "Statistics.html#advanced-statistical-methods",
    "title": "Stastistics",
    "section": "",
    "text": "Survival Analysis in R\n\nFunction: Time-to-event analysis, Kaplan-Meier curves, Cox regression, and survival modeling using R survival packages\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFundamentals of Bayesian Data Analysis in R\n\nFunction: Bayesian inference, MCMC methods, prior distributions, and posterior analysis using R and Stan/JAGS\nWebsite: https://www.datacamp.com\nCost Model: Subscription\n\nFactor Analysis in R\n\nFunction: Exploratory and confirmatory factor analysis, dimensionality reduction, and latent variable modeling in R\nWebsite: https://www.datacamp.com\nCost Model: Subscription"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "Data_Analyst.html",
    "href": "Data_Analyst.html",
    "title": "Data Anlyst",
    "section": "",
    "text": "Este documento demuestra cómo crear gráficos de barras utilizando Python (Matplotlib), SQL para consultas de datos, y R (ggplot2) para visualizaciones. Esta aproximación multilenguaje permite aprovechar las fortalezas específicas de cada herramienta."
  },
  {
    "objectID": "Data_Analyst.html#introducción",
    "href": "Data_Analyst.html#introducción",
    "title": "Data Anlyst",
    "section": "",
    "text": "Este documento demuestra cómo crear gráficos de barras utilizando Python (Matplotlib), SQL para consultas de datos, y R (ggplot2) para visualizaciones. Esta aproximación multilenguaje permite aprovechar las fortalezas específicas de cada herramienta."
  },
  {
    "objectID": "Data_Analyst.html#bar-charts---python-matplotlib",
    "href": "Data_Analyst.html#bar-charts---python-matplotlib",
    "title": "Data Anlyst",
    "section": "Bar Charts - Python (Matplotlib)",
    "text": "Bar Charts - Python (Matplotlib)\n\nGráfico Básico de Barras Verticales\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Datos de lenguajes de programación\neje_x = ['Python', 'R', 'Node.js', 'PHP', 'Java', 'JavaScript']\neje_y = [50, 20, 35, 47, 42, 55]\n\nplt.figure(figsize=(12, 7))\ncolors = ['#3776ab', '#276DC3', '#8CC84B', '#777BB4', '#ED8B00', '#F7DF1E']\nbars = plt.bar(eje_x, eje_y, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n\n# Agregar valores encima de las barras\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n             f'{int(height)}K', ha='center', va='bottom', fontweight='bold')\n\nplt.ylabel('Cantidad de usuarios (miles)', fontsize=12)\nplt.xlabel('Lenguajes de programación', fontsize=12)\nplt.title('Popularidad de Lenguajes de Programación', fontsize=14, fontweight='bold')\nplt.xticks(rotation=45)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGráfico de Barras Horizontales\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Datos de habilidades profesionales\nhabilidades = ['Programación', 'Ciencia de Datos', 'Matemáticas', 'Ingeniería', 'Diseño', 'Marketing']\nempleados = [76, 31, 45, 57, 38, 29]\n\nplt.figure(figsize=(12, 8))\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']\nbars = plt.barh(habilidades, empleados, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n\n# Agregar valores al final de las barras\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    plt.text(width + 1, bar.get_y() + bar.get_height()/2.,\n             f'{int(width)}', ha='left', va='center', fontweight='bold')\n\nplt.xlabel('Número de empleados', fontsize=12)\nplt.ylabel('Habilidades', fontsize=12)\nplt.title('Distribución de Habilidades en el Equipo', fontsize=14, fontweight='bold')\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGráfico de Barras Agrupadas\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Datos de múltiples series\naños = ['2020', '2021', '2022', '2023', '2024']\npython = [45, 50, 55, 60, 65]\nr = [15, 20, 22, 25, 28]\njavascript = [40, 45, 50, 55, 60]\n\nx = np.arange(len(años))\nwidth = 0.25\n\nplt.figure(figsize=(12, 8))\nbars1 = plt.bar(x - width, python, width, label='Python', color='#3776ab', alpha=0.8)\nbars2 = plt.bar(x, r, width, label='R', color='#276DC3', alpha=0.8)\nbars3 = plt.bar(x + width, javascript, width, label='JavaScript', color='#F7DF1E', alpha=0.8)\n\nplt.xlabel('Años', fontsize=12)\nplt.ylabel('Usuarios (miles)', fontsize=12)\nplt.title('Evolución de Usuarios por Lenguaje (2020-2024)', fontsize=14, fontweight='bold')\nplt.xticks(x, años)\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Data_Analyst.html#sql---consultas-para-análisis-de-datos",
    "href": "Data_Analyst.html#sql---consultas-para-análisis-de-datos",
    "title": "Data Anlyst",
    "section": "SQL - Consultas para Análisis de Datos",
    "text": "SQL - Consultas para Análisis de Datos\n\nConsulta de Datos de Lenguajes\n\n\nCode\nSELECT \n    lenguaje,\n    usuarios,\n    ROUND(usuarios * 100.0 / (SELECT SUM(usuarios) FROM ventas_programacion), 1) as porcentaje_mercado\nFROM ventas_programacion \nORDER BY usuarios DESC;\n\n\n\n6 records\n\n\nlenguaje\nusuarios\nporcentaje_mercado\n\n\n\n\nJavaScript\n55\n22.1\n\n\nPython\n50\n20.1\n\n\nPHP\n47\n18.9\n\n\nJava\n42\n16.9\n\n\nNode.js\n35\n14.1\n\n\nR\n20\n8.0\n\n\n\n\n\n\n\nTop 3 Lenguajes Más Populares\n\n\nCode\nSELECT \n    lenguaje,\n    usuarios,\n    CASE \n        WHEN usuarios &gt;= 50 THEN 'Alta Popularidad'\n        WHEN usuarios &gt;= 35 THEN 'Popularidad Media'\n        ELSE 'Popularidad Baja'\n    END as categoria_popularidad\nFROM ventas_programacion \nORDER BY usuarios DESC \nLIMIT 3;\n\n\n\n3 records\n\n\nlenguaje\nusuarios\ncategoria_popularidad\n\n\n\n\nJavaScript\n55\nAlta Popularidad\n\n\nPython\n50\nAlta Popularidad\n\n\nPHP\n47\nPopularidad Media\n\n\n\n\n\n\n\nAnálisis de Habilidades con Estadísticas\n\n\nCode\nSELECT \n    habilidad,\n    cantidad,\n    ROUND(AVG(cantidad) OVER(), 1) as promedio_general,\n    ROUND(cantidad - AVG(cantidad) OVER(), 1) as diferencia_promedio,\n    CASE \n        WHEN cantidad &gt; AVG(cantidad) OVER() THEN 'Por encima del promedio'\n        ELSE 'Por debajo del promedio'\n    END as clasificacion\nFROM empleados_habilidades \nORDER BY cantidad DESC;\n\n\n\n6 records\n\n\n\n\n\n\n\n\n\nhabilidad\ncantidad\npromedio_general\ndiferencia_promedio\nclasificacion\n\n\n\n\nProgramación\n76\n46\n30\nPor encima del promedio\n\n\nIngeniería\n57\n46\n11\nPor encima del promedio\n\n\nMatemáticas\n45\n46\n-1\nPor debajo del promedio\n\n\nDiseño\n38\n46\n-8\nPor debajo del promedio\n\n\nCiencia de Datos\n31\n46\n-15\nPor debajo del promedio\n\n\nMarketing\n29\n46\n-17\nPor debajo del promedio\n\n\n\n\n\n\n\nEvolución Temporal de Lenguajes\n\n\nCode\nSELECT \n    año,\n    python,\n    r,\n    javascript,\n    (python + r + javascript) as total_usuarios,\n    ROUND(python * 100.0 / (python + r + javascript), 1) as porcentaje_python\nFROM ventas_por_año \nORDER BY año;\n\n\n\n5 records\n\n\naño\npython\nr\njavascript\ntotal_usuarios\nporcentaje_python\n\n\n\n\n2020\n45\n15\n40\n100\n45.0\n\n\n2021\n50\n20\n45\n115\n43.5\n\n\n2022\n55\n22\n50\n127\n43.3\n\n\n2023\n60\n25\n55\n140\n42.9\n\n\n2024\n65\n28\n60\n153\n42.5"
  },
  {
    "objectID": "Data_Analyst.html#bar-charts---r-ggplot2",
    "href": "Data_Analyst.html#bar-charts---r-ggplot2",
    "title": "Data Anlyst",
    "section": "Bar Charts - R (ggplot2)",
    "text": "Bar Charts - R (ggplot2)\n\nGráfico Básico con R\n\n\nCode\n# Obtener datos de SQL\ndatos_lenguajes &lt;- dbGetQuery(con, \"SELECT lenguaje, usuarios FROM ventas_programacion ORDER BY usuarios DESC\")\n\n# Crear gráfico con ggplot2\nggplot(datos_lenguajes, aes(x = reorder(lenguaje, usuarios), y = usuarios, fill = lenguaje)) +\n  geom_bar(stat = \"identity\", alpha = 0.8, color = \"black\", size = 0.5) +\n  geom_text(aes(label = paste0(usuarios, \"K\")), hjust = -0.1, fontface = \"bold\") +\n  coord_flip() +\n  labs(title = \"Popularidad de Lenguajes de Programación\",\n       subtitle = \"Datos obtenidos de consulta SQL\",\n       x = \"Lenguajes de Programación\", \n       y = \"Usuarios (miles)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 16, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        axis.text = element_text(size = 11),\n        axis.title = element_text(size = 12, face = \"bold\")) +\n  scale_fill_viridis_d(option = \"plasma\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1)))\n\n\n\n\n\n\n\n\n\n\n\nGráfico de Habilidades\n\n\nCode\n# Obtener datos de habilidades\ndatos_habilidades &lt;- dbGetQuery(con, \"SELECT habilidad, cantidad FROM empleados_habilidades ORDER BY cantidad DESC\")\n\nggplot(datos_habilidades, aes(x = reorder(habilidad, cantidad), y = cantidad, fill = cantidad)) +\n  geom_bar(stat = \"identity\", alpha = 0.8, color = \"black\", size = 0.5) +\n  geom_text(aes(label = cantidad), hjust = -0.1, fontface = \"bold\") +\n  coord_flip() +\n  labs(title = \"Distribución de Habilidades en el Equipo\",\n       subtitle = \"Número de empleados por habilidad\",\n       x = \"Habilidades\", \n       y = \"Número de empleados\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 16, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        axis.text = element_text(size = 11),\n        axis.title = element_text(size = 12, face = \"bold\")) +\n  scale_fill_gradient(low = \"#E8F4FD\", high = \"#1E88E5\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1)))\n\n\n\n\n\n\n\n\n\n\n\nGráfico de Evolución Temporal\n\n\nCode\n# Obtener datos temporales\ndatos_tiempo &lt;- dbGetQuery(con, \"SELECT * FROM ventas_por_año ORDER BY año\")\n\n# Transformar datos para ggplot\ndatos_long &lt;- pivot_longer(datos_tiempo, \n                          cols = c(python, r, javascript), \n                          names_to = \"lenguaje\", \n                          values_to = \"usuarios\")\n\n# Crear gráfico\nggplot(datos_long, aes(x = factor(año), y = usuarios, fill = lenguaje)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.8, color = \"black\", size = 0.3) +\n  geom_text(aes(label = paste0(usuarios, \"K\")), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, \n            fontface = \"bold\", \n            size = 3) +\n  labs(title = \"Evolución de Usuarios por Lenguaje (2020-2024)\",\n       subtitle = \"Crecimiento anual de las principales tecnologías\",\n       x = \"Años\", \n       y = \"Usuarios (miles)\",\n       fill = \"Lenguaje\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        axis.text = element_text(size = 11),\n        axis.title = element_text(size = 12, face = \"bold\"),\n        legend.title = element_text(size = 12, face = \"bold\"),\n        legend.text = element_text(size = 11)) +\n  scale_fill_manual(values = c(\"python\" = \"#3776ab\", \"r\" = \"#276DC3\", \"javascript\" = \"#F7DF1E\")) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1)))"
  },
  {
    "objectID": "Data_Analyst.html#integración-sql-python",
    "href": "Data_Analyst.html#integración-sql-python",
    "title": "Data Anlyst",
    "section": "Integración SQL + Python",
    "text": "Integración SQL + Python\nCombinemos una consulta SQL compleja con visualización en Python:\n\n\nCode\n# Ejecutar consulta SQL avanzada\ndatos_analisis &lt;- dbGetQuery(con, \"\nSELECT \n    lenguaje,\n    usuarios,\n    CASE \n        WHEN usuarios &gt;= 50 THEN 'Alto'\n        WHEN usuarios &gt;= 35 THEN 'Medio'\n        ELSE 'Bajo'\n    END as categoria,\n    ROUND(usuarios * 100.0 / (SELECT SUM(usuarios) FROM ventas_programacion), 1) as porcentaje\nFROM ventas_programacion \nWHERE usuarios &gt; 25\nORDER BY usuarios DESC\n\")\n\n# Pasar datos a Python\npy$datos_desde_r &lt;- datos_analisis\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Usar datos de R/SQL\ndf = pd.DataFrame(datos_desde_r)\n\n# Crear subplot con dos gráficos\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Gráfico 1: Barras por usuarios\ncolors = ['#FF6B6B' if cat == 'Alto' else '#4ECDC4' if cat == 'Medio' else '#45B7D1' \n          for cat in df['categoria']]\n\nbars1 = ax1.bar(df['lenguaje'], df['usuarios'], color=colors, alpha=0.8, edgecolor='black')\nax1.set_title('Lenguajes por Número de Usuarios', fontweight='bold')\nax1.set_xlabel('Lenguajes')\nax1.set_ylabel('Usuarios (miles)')\nax1.tick_params(axis='x', rotation=45)\n\n# Agregar valores en las barras\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n             f'{int(height)}K', ha='center', va='bottom', fontweight='bold')\n\n# Gráfico 2: Distribución por porcentaje\nwedges, texts, autotexts = ax2.pie(df['porcentaje'], labels=df['lenguaje'], autopct='%1.1f%%',\n                                   colors=colors, startangle=90)\nax2.set_title('Distribución de Mercado', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Data_Analyst.html#análisis-comparativo-final",
    "href": "Data_Analyst.html#análisis-comparativo-final",
    "title": "Data Anlyst",
    "section": "Análisis Comparativo Final",
    "text": "Análisis Comparativo Final\n\nResumen de Datos por Categoría\n\n\nCode\nSELECT \n    categoria,\n    COUNT(*) as cantidad_lenguajes,\n    AVG(usuarios) as promedio_usuarios,\n    SUM(usuarios) as total_usuarios\nFROM (\n    SELECT \n        lenguaje,\n        usuarios,\n        CASE \n            WHEN usuarios &gt;= 50 THEN 'Alto'\n            WHEN usuarios &gt;= 35 THEN 'Medio'\n            ELSE 'Bajo'\n        END as categoria\n    FROM ventas_programacion\n) \nGROUP BY categoria\nORDER BY total_usuarios DESC;\n\n\n\n3 records\n\n\ncategoria\ncantidad_lenguajes\npromedio_usuarios\ntotal_usuarios\n\n\n\n\nMedio\n3\n41.33333\n124\n\n\nAlto\n2\n52.50000\n105\n\n\nBajo\n1\n20.00000\n20"
  },
  {
    "objectID": "Automotion&DEV.html",
    "href": "Automotion&DEV.html",
    "title": "Automation & Software Development",
    "section": "",
    "text": "Function: Microsoft tool for automating workflows and connecting different applications and services within the Microsoft ecosystem.\nOfficial Website: https://powerautomate.microsoft.com\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction: Visual automation platform that connects over 1,000 applications using flowchart diagrams and advanced data transformations.\nOfficial Website: https://www.make.com\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction: Open-source automation tool that allows creating complex workflows with a visual node-based interface and self-hosting capabilities.\nOfficial Website: https://n8n.io"
  },
  {
    "objectID": "Automotion&DEV.html#power-automate",
    "href": "Automotion&DEV.html#power-automate",
    "title": "Automation & Software Development",
    "section": "",
    "text": "Function: Microsoft tool for automating workflows and connecting different applications and services within the Microsoft ecosystem.\nOfficial Website: https://powerautomate.microsoft.com"
  },
  {
    "objectID": "Automotion&DEV.html#make",
    "href": "Automotion&DEV.html#make",
    "title": "Automation & Software Development",
    "section": "",
    "text": "Function: Visual automation platform that connects over 1,000 applications using flowchart diagrams and advanced data transformations.\nOfficial Website: https://www.make.com"
  },
  {
    "objectID": "Automotion&DEV.html#n8n",
    "href": "Automotion&DEV.html#n8n",
    "title": "Automation & Software Development",
    "section": "",
    "text": "Function: Open-source automation tool that allows creating complex workflows with a visual node-based interface and self-hosting capabilities.\nOfficial Website: https://n8n.io"
  },
  {
    "objectID": "Automotion&DEV.html#power-apps",
    "href": "Automotion&DEV.html#power-apps",
    "title": "Automation & Software Development",
    "section": "Power Apps",
    "text": "Power Apps\n\n\n\n\n\n\n\n\n\nFunction: Microsoft’s low-code platform for creating business applications without advanced programming, integrated with Microsoft 365.\nOfficial Website: https://powerapps.microsoft.com"
  },
  {
    "objectID": "Automotion&DEV.html#bubble.io",
    "href": "Automotion&DEV.html#bubble.io",
    "title": "Automation & Software Development",
    "section": "Bubble.io",
    "text": "Bubble.io\n\n\n\n\n\n\n\n\n\nFunction: No-code platform for building web applications with visual programming, database management, and hosting included.\nOfficial Website: https://bubble.io"
  },
  {
    "objectID": "Automotion&DEV.html#flutter",
    "href": "Automotion&DEV.html#flutter",
    "title": "Automation & Software Development",
    "section": "Flutter",
    "text": "Flutter\n\n\n\n\n\n\n\n\n\nFunction: Google’s UI toolkit for building natively compiled applications for mobile, web, and desktop from a single codebase using Dart language.\nOfficial Website: https://flutter.dev"
  },
  {
    "objectID": "Automotion&DEV.html#airtable",
    "href": "Automotion&DEV.html#airtable",
    "title": "Automation & Software Development",
    "section": "Airtable",
    "text": "Airtable\n\n\n\n\n\n\n\n\n\nFunction: Collaborative database platform that combines spreadsheets with automation capabilities, multiple views, and application development features.\nOfficial Website: https://www.airtable.com"
  },
  {
    "objectID": "about-es.html",
    "href": "about-es.html",
    "title": "Luis Carreño",
    "section": "",
    "text": "English  Português  Español\n\n\nFormación académica: Profesional en tecnología con formación en Ingeniería Industrial por la UNI y especialización en Logística por ESAN y TEC de Monterrey.\nExperiencia: Más de 7 años resolviendo problemas complejos mediante automatización, análisis de datos,ingeniería de datos y visualización.\nProgramming Languages : Python | Java | C++ | SQL | HTML and CSS.\nAutomatización y Desarrollo: Power Automate | Make | N8N | Power Apps | Glide | Bubble | Flutter | Airtable\nSoftware & Tools : Power query | R | Excel | Big Query | PostgreSQL | Power BI |Tableau | Looker Studio | Google Cloud | OpenAI API | AI Google Studio | LangChain | AWS."
  },
  {
    "objectID": "AI_Engineer.html",
    "href": "AI_Engineer.html",
    "title": "AI Engineer",
    "section": "",
    "text": "This document outlines essential tools and a structured learning path for AI Engineers, with a focus on Large Language Models (LLMs), AI agents, and scalable AI systems.\n\n\n\n\n\nFunction: General-purpose programming language with powerful libraries for AI, machine learning, and deep learning (TensorFlow, PyTorch, Keras, scikit-learn). Essential for developing and deploying AI models and intelligent agents.\nWebsite: Python.org\nCost Model: Open Source\nBest For: Machine Learning, Deep Learning, Natural Language Processing, Agent Development, Automation, Production Deployment\n\n\n\n\n\n\n\n\nFunction: Framework for developing applications powered by language models. Provides tools for chaining components to build complex, contextual AI systems, especially for agents and Retrieval Augmented Generation (RAG).\nWebsite: LangChain.com\nCost Model: Open Source\nBest For: Large Language Model (LLM) Applications, AI Agents, Chatbots, Contextual AI\n\n\n\n\n\nFunction: Access to powerful AI models (e.g., GPT series, DALL-E) for natural language processing and image generation. Critical for integrating state-of-the-art AI into applications.\nWebsite: OpenAI API\nCost Model: Paid (usage-based)\nBest For: Advanced LLMs, Generative AI, Conversational AI\n\n\n\n\n\nFunction: Open-source platform for building conversational AI applications and chatbots. Offers tools for designing conversational flows and integrating with messaging channels.\nWebsite: Botpress.com\nCost Model: Open Source (with enterprise options)\nBest For: Chatbot Development, Conversational AI, Customer Service Automation\n\n\n\n\n\nFunction: Platform for debugging, testing, evaluating, and monitoring LLM applications built with LangChain. Essential for MLOps in LLM development.\nWebsite: LangSmith\nCost Model: Paid\nBest For: LLM Application Development, Debugging, Evaluation, Monitoring\n\n\n\n\n\nFunction: Framework for orchestrating role-playing autonomous AI agents. Allows defining agents with specific roles, goals, and tools for collaborative problem-solving.\nWebsite: Part of the LangChain/AI agent ecosystem\nCost Model: Open Source (Python library)\nBest For: Multi-Agent Systems, Complex Problem Solving, Autonomous Agents\n\n\n\n\n\nFunction: Lightweight Python web framework for developing web applications and APIs to serve AI models or interact with AI services. Ideal for chatbot or RAG endpoints.\nWebsite: Flask\nCost Model: Open Source\nBest For: Web Application Development, API Endpoints for AI Services, Rapid Prototyping\n\n\n\n\n\n\n\n\nFunction: Suite of cloud services with robust AI/ML offerings like Vertex AI, Dialogflow, and Cloud Storage. Provides scalable infrastructure for training, deploying, and managing AI models.\nWebsite: Google Cloud\nCost Model: Paid (pay-as-you-go)\nBest For: Scalable AI/ML Workflows, Model Deployment, MLOps, Data Storage\n\n\n\n\n\nFunction: Unified ML platform on GCP for building, deploying, and scaling ML models. Includes tools for data preparation, training, deployment, and monitoring.\nWebsite: Vertex AI\nCost Model: Paid (usage-based)\nBest For: End-to-end ML Lifecycle Management, MLOps, Scalable Model Deployment\n\n\n\n\n\nFunction: Natural language understanding (NLU) platform for building conversational interfaces like chatbots and voice assistants. Integrates with messaging platforms.\nWebsite: Dialogflow\nCost Model: Paid (usage-based)\nBest For: Conversational AI, Chatbot Development, Virtual Agents\n\n\n\n\n\n\n\n\nFunction: Cloud-based spreadsheet for data organization and as a data source for AI applications, particularly for managing prompt templates or small datasets.\nWebsite: Google Sheets\n**Cost Model"
  },
  {
    "objectID": "AI_Engineer.html#core-programming-languages",
    "href": "AI_Engineer.html#core-programming-languages",
    "title": "AI Engineer",
    "section": "",
    "text": "Function: General-purpose programming language with powerful libraries for AI, machine learning, and deep learning (TensorFlow, PyTorch, Keras, scikit-learn). Essential for developing and deploying AI models and intelligent agents.\nWebsite: Python.org\nCost Model: Open Source\nBest For: Machine Learning, Deep Learning, Natural Language Processing, Agent Development, Automation, Production Deployment"
  },
  {
    "objectID": "AI_Engineer.html#ai-machine-learning-frameworkslibraries",
    "href": "AI_Engineer.html#ai-machine-learning-frameworkslibraries",
    "title": "AI Engineer",
    "section": "",
    "text": "Function: Framework for developing applications powered by language models. Provides tools for chaining components to build complex, contextual AI systems, especially for agents and Retrieval Augmented Generation (RAG).\nWebsite: LangChain.com\nCost Model: Open Source\nBest For: Large Language Model (LLM) Applications, AI Agents, Chatbots, Contextual AI\n\n\n\n\n\nFunction: Access to powerful AI models (e.g., GPT series, DALL-E) for natural language processing and image generation. Critical for integrating state-of-the-art AI into applications.\nWebsite: OpenAI API\nCost Model: Paid (usage-based)\nBest For: Advanced LLMs, Generative AI, Conversational AI\n\n\n\n\n\nFunction: Open-source platform for building conversational AI applications and chatbots. Offers tools for designing conversational flows and integrating with messaging channels.\nWebsite: Botpress.com\nCost Model: Open Source (with enterprise options)\nBest For: Chatbot Development, Conversational AI, Customer Service Automation\n\n\n\n\n\nFunction: Platform for debugging, testing, evaluating, and monitoring LLM applications built with LangChain. Essential for MLOps in LLM development.\nWebsite: LangSmith\nCost Model: Paid\nBest For: LLM Application Development, Debugging, Evaluation, Monitoring\n\n\n\n\n\nFunction: Framework for orchestrating role-playing autonomous AI agents. Allows defining agents with specific roles, goals, and tools for collaborative problem-solving.\nWebsite: Part of the LangChain/AI agent ecosystem\nCost Model: Open Source (Python library)\nBest For: Multi-Agent Systems, Complex Problem Solving, Autonomous Agents\n\n\n\n\n\nFunction: Lightweight Python web framework for developing web applications and APIs to serve AI models or interact with AI services. Ideal for chatbot or RAG endpoints.\nWebsite: Flask\nCost Model: Open Source\nBest For: Web Application Development, API Endpoints for AI Services, Rapid Prototyping"
  },
  {
    "objectID": "AI_Engineer.html#cloud-ai-platforms",
    "href": "AI_Engineer.html#cloud-ai-platforms",
    "title": "AI Engineer",
    "section": "",
    "text": "Function: Suite of cloud services with robust AI/ML offerings like Vertex AI, Dialogflow, and Cloud Storage. Provides scalable infrastructure for training, deploying, and managing AI models.\nWebsite: Google Cloud\nCost Model: Paid (pay-as-you-go)\nBest For: Scalable AI/ML Workflows, Model Deployment, MLOps, Data Storage\n\n\n\n\n\nFunction: Unified ML platform on GCP for building, deploying, and scaling ML models. Includes tools for data preparation, training, deployment, and monitoring.\nWebsite: Vertex AI\nCost Model: Paid (usage-based)\nBest For: End-to-end ML Lifecycle Management, MLOps, Scalable Model Deployment\n\n\n\n\n\nFunction: Natural language understanding (NLU) platform for building conversational interfaces like chatbots and voice assistants. Integrates with messaging platforms.\nWebsite: Dialogflow\nCost Model: Paid (usage-based)\nBest For: Conversational AI, Chatbot Development, Virtual Agents"
  },
  {
    "objectID": "AI_Engineer.html#data-storage-utilities",
    "href": "AI_Engineer.html#data-storage-utilities",
    "title": "AI Engineer",
    "section": "",
    "text": "Function: Cloud-based spreadsheet for data organization and as a data source for AI applications, particularly for managing prompt templates or small datasets.\nWebsite: Google Sheets\n**Cost Model"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome 2/index.html",
    "href": "posts/welcome 2/index.html",
    "title": "Welcome To My Blog 2",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]